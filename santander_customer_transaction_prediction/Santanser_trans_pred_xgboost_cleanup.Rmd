---
title: "Santandar Costumer Transaction Prediction with xgboost (cleanup)"
author: "Karim Mezhoud"
date: '`r Sys.Date()`'
output:
  html_document:
    number_sections: true
    fig_caption: true
    toc: true
    fig_width: 7
    fig_height: 6
    theme: cosmo
    highlight: tango
    code_folding: show #hide
    self_contained: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE)
```

# Load packages
```{r comment=FALSE, warning=FALSE}
library(data.table)
library(dplyr)
library(rsample)
library(xgboost)
require(Ckmeans.1d.dp)
library(ggplot2)
```
# Load Data

```{r}
train <- fread(file = "train.csv")
train[1:10,1:14]
```
# Split Into Train/Test Sets
```{r}
set.seed(100)
train_test_split <- rsample::initial_split(train, prop = 0.8)

train_8 <- rsample::training(train_test_split)
test_2  <- rsample::testing(train_test_split)

```

# Optimize features with Cross validation

Here, we can see after how many rounds, we achieved the smallest test error.
```{r}
train <- train[,-1]
train_sparse <- Matrix::sparse.model.matrix(target ~., data=train)
dtrain <- xgb.DMatrix(data=train_sparse, label = train$target)

#dtrain <- xgb.DMatrix(as.matrix(train_8[, -c(1,2)]), 
#                      label = as.numeric(train_8$target))
#dtest <- xgb.DMatrix(as.matrix(test_2[, -c(1,2)]), 
#                      label = as.numeric(test_2$target))


params <- list(booster = "gbtree",
               tree_method = "auto",
              objective = "binary:logistic",
              eval_metric = "auc",         #  for Binary classification error rate
              max_depth = 2,                 # 6 makes training heavy, there is no correlation between features
              eta = 0.01,                     # learning rate
              subsample = 0.5,              # prevent overfitting
              colsample_bytree = 0.1         # specify the fraction of columns to be subsampled.
             )

#watchlist <- list(train = dtrain, eval = dtest)


tme <- Sys.time()
cv_model <- xgb.cv(params = params,
                   data = dtrain,
                   nthread = 2,
                   nrounds = 150,
                   verbose = TRUE,
                   nfold = 5,
                   early_stopping_rounds = 8,
                   maximize = TRUE,
                   prediction = TRUE) # prediction of cv folds
Sys.time() - tme
cv_model$best_iteration
```



# Train the model
```{r}
tme <- Sys.time()
xgboost_tree <- xgb.train(data = dtrain, 
                          #params = params,
                         nrounds = 10, #cv_model$best_iteration,
                         print_every_n = 100,
                         verbose = TRUE)
Sys.time() - tme
```

## testing Tree model
```{r}
pred_tree_2 <- predict(xgboost_tree, as.matrix(test_2[, c(-1,-2)]))
head(pred_tree_2)
```

## Confusion matric for Tree model
```{r}
data.frame(prediction = as.numeric(prediction_tree_2),
         label = as.numeric(test_2$target)) %>%
  count(prediction, label)
```

# Prediction

## Load test data
```{r}
test <- fread(file = "test.csv")
test[1:10,1:14]
```
## Prediction with Tree xgboost model

```{r}
pred_tree <- predict(xgboost_tree, as.matrix(test[,-1]))
head(pred_tree)
```

## Glimpse proportion 1/0
```{r}
prediction_tree <- as.numeric(pred_tree > 0.5)
table(prediction_tree)
```
# submission

```{r}
dt_submission <- data.frame(
  ID_code = ID_code,
  target = pred_tree
)
head(dt_submission)
```

```{r}
sub_tmp <- fread(file = "submission_cleanup.csv")
sub_tmp <- sub_tmp %>%
  mutate(var_0 = NULL) %>%
  mutate(ID_code = test$ID_code, target = target)
#fwrite(sub_tmp, "submission_cleanup.csv")
```

