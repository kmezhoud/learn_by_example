---
title: "Santandar Costumer Transaction Prediction with xgboost (cleanup)"
author: "Karim Mezhoud"
date: '`r Sys.Date()`'
output:
  html_document:
    number_sections: true
    fig_caption: true
    toc: true
    fig_width: 7
    fig_height: 6
    theme: cosmo
    highlight: tango
    code_folding: show #hide
    self_contained: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE)
```

# Load packages
```{r comment=FALSE, warning=FALSE}
library(data.table)
library(dplyr)
library(rsample)
library(xgboost)
require(Ckmeans.1d.dp)
library(ggplot2)
library(Matrix)
```
# Load train Data and format to DMatrix

```{r}
train <- fread(file = "train.csv")
train[1:10,1:14]

```
# Split Into Train/Test Sets
```{r}
set.seed(100)
train_test_split <- rsample::initial_split(train, prop = 0.8)
train_test_split
```

We can retrieve our training and testing sets using training() and testing() functions.

```{r}
# Retrieve train and test sets
train_8 <- rsample::training(train_test_split)
test_2  <- rsample::testing(train_test_split)
train_8[1:10, 1:14]
```


## format train and test to DMatrix 
```{r}
train_8$ID_code <- NULL
train_8_sparse <- sparse.model.matrix(target ~., data=train_8)
dtrain_8 <- xgb.DMatrix(data=train_8_sparse, label = train_8$target)

test_2$ID_code <- NULL
test_2_sparse <- sparse.model.matrix(target ~., data=test_2)
dtest_2 <- xgb.DMatrix(data=test_2_sparse, label = test_2$target)
```




# Optimize features with Cross validation

Here, we can see after how many rounds, we achieved the smallest test error.
```{r}

params <- list(booster = "gbtree",
               tree_method = "auto",
              objective = "binary:logistic",
              eval_metric = "auc",         #  for Binary classification error rate
              max_depth = 2,                 # 6 makes training heavy, there is no correlation between features
              eta = 0.01,                     # learning rate
              subsample = 0.5,              # prevent overfitting
              colsample_bytree = 0.1         # specify the fraction of columns to be subsampled.
             )


tme <- Sys.time()
cv_model <- xgb.cv(params = params,
                   data = dtrain_8,
                   nthread = 2,
                   nrounds = 30,
                   verbose = TRUE,
                   nfold = 5,
                   print_every_n = 100,
                   early_stopping_rounds = 500,
                   maximize = TRUE,
                   prediction = TRUE) # prediction of cv folds
Sys.time() - tme
```



# Train the model
```{r}
watchlist <- list(train = dtrain_8, eval = dtest_2)
tme <- Sys.time()
xgboost_tree <- xgb.train(data = dtrain_8, 
                         params = params,
                         watchlist = watchlist,
                         nrounds = cv_model$best_iteration,
                         print_every_n = 1,
                         verbose = TRUE)
Sys.time() - tme
```


# Prediction
## Load test data and format to DMatrix
```{r}
test <- data.table::fread(file = "test.csv")
ID_code <- test$ID_code
test$ID_code <- NULL
test_sparse <- sparse.model.matrix(ID_code ~., data=test)
 dtest <- xgb.DMatrix(data=test_sparse, label = ID_code)

```
## Prediction with Tree xgboost model

```{r}
pred_tree <- predict(xgboost_tree, dtest)
head(pred_tree)
```

## summarize probabilities of targets
```{r}
summary(pred_tree)
```
# submission

```{r}
dt_submission <- data.frame(
  ID_code = ID_code,
  target = pred_tree
)
summary(dt_submission)
```

```{r}
fwrite(dt_submission, "dt_submission.csv")
```

