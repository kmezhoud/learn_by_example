---
title: 'Setting multiple Spark Nodes '
output:
    html_document:
    number_sections: true
    fig_caption: true
    toc: true
    fig_width: 7
    fig_height: 4.5
    theme: cosmo
    highlight: tango
    #code_folding: hide
    html_notebook: null
    df_print: paged 
---

## which versions installed?
```{r}
library(sparklyr)
spark_installed_versions()
```

## Last Spark version Availability
```{r}
spark_available_versions(show_hadoop = TRUE) %>% tail()
```

## Install Specific spark version
```{r}
library(sparklyr)
#spark_install(version = "2.4.0")
```


## Connect to two spark versions

```{r}
conf232 <- spark_config()   # Load variable with spark_config()
conf232$`sparklyr.cores.local` <- 2
conf232$`sparklyr.shell.driver-memory` <- "4G"
conf232$spark.memory.fraction <- 0.5

conf240 <- spark_config()   # Load variable with spark_config()
conf240$`sparklyr.cores.local` <- 2
conf240$`sparklyr.shell.driver-memory` <- "4G"
conf240$spark.memory.fraction <- 0.5

sc232 <- spark_connect(master = "local",
                       spark_home = "/Users/Mezhoud/spark/spark-2.3.2-bin-hadoop2.7",
                       version = "2.3.2",
                       config = conf232
                    )

sc240 <- spark_connect(master = "local",
                       spark_home = "/Users/Mezhoud/spark/spark-2.4.0-bin-hadoop2.7",
                       version = "2.4.0",
                       config = conf240
                    )


spark_version(sc240)
spark_version(sc232)
```

Only the first connection is done!

## Copy data set in different nodes

.... working....
